{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础理论部分\n",
    "0. Can you come up out 3 sceneraies which use AI methods?\n",
    "\n",
    "Ans: {自动驾驶、智能家居、各类机器人}\n",
    "\n",
    "1. How do we use Github; Why do we use Jupyter and Pycharm;\n",
    "\n",
    "Ans: {github: 1. git add .;  2. git commit -m '123'; 3. git push origin your_branch 4.git pull oriign your_branch; 5 git clone git_address\n",
    "\n",
    "jupyter: 使用方便，简单直观可视化效果好。\n",
    "pyharm：强大的IDE, 貌似绝大部分的python开发者都用pycharm。}\n",
    "\n",
    "2. What's the Probability Model?\n",
    "\n",
    "Ans: 概率模型是指定随机变量出现的不确定性的概率。通过样本发生的概率判断其是否发生或产生相对正确的结果\n",
    "\n",
    "3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "\n",
    "Ans: 车牌摇号、文字纠错、谷歌搜索错误单词匹配、文本分类(垃圾邮件分类/文本分类等)(比如 jult --提示--> july)\n",
    "\n",
    "4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "\n",
    "Ans: 语言是千变万化、与时俱进的，使用正则匹配和语法无法满足现实需求或者是无法投入商业使用，而且后两者维护起来相对麻烦，对出现的新的语言或语句没有很好的处理方式\n",
    "\n",
    "5. What's the Language Model;\n",
    "\n",
    "Ans: 语言模型基于概率模型，在大量语料的基础上通过概率模型判断某一语句发生的可能性\n",
    "\n",
    "6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "\n",
    "Ans: 阿里/JD智能客服、语音识别、机器翻译\n",
    "\n",
    "7. What's the 1-gram language model;\n",
    "\n",
    "Ans: 一般在处理语言相关数据或模型时，可能特征值比较多，这是假设第N个特征值的出现只与其本身存在关系\n",
    "\n",
    "8. What's the disadvantages and advantages of 1-gram language model;\n",
    "\n",
    "Ans: 1-gram 适用于特征之间相关性比较弱的模型，如果相关性很强，那么1-gram的假设条件就会出现不比较大的误差\n",
    "\n",
    "9. What't the 2-gram models;\n",
    "Ans: 2-gram是在多个特征值出现的情况下，当前特征n出现的概率至于其下一个特征n+1有关，与其他的特征值无关\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_rules = \"\"\"\n",
    "host = 主演 电影 评论\n",
    "主演 = 吴京 adj | 王宝强 adj | 周润发 adj\n",
    "adj = 的 | 这部 | 这个 | 这辆\n",
    "电影 = 杀破狼 | 战狼 | 唐人街探案 | 赌神 |\n",
    "评论 = 太好了 | 垃圾 | 恶心 | 好吃 | 车 adj | 稳住 | 不错\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammer(gram, line_split='\\n', word_split='='):\n",
    "    grammer = {}\n",
    "    for line in gram.split(line_split):\n",
    "        if not line: continue\n",
    "        key, value = line.split(word_split)\n",
    "        grammer[''.join(key.split())] = [val.split() for val in value.split('|')]\n",
    "    return grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(sentence, target):\n",
    "    grammer = create_grammer(sentence)\n",
    "    if target not in grammer: \n",
    "        return target\n",
    "    sen = [generate(sentence, t) for t in random.choice(grammer[target])]\n",
    "    return ''.join(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(sentence, target, n):\n",
    "    for i in range(n):\n",
    "        yield generate(sentence, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_txt(string):\n",
    "    return re.findall('\\w+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cut(string):\n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_gram(path='word_gram.txt'):\n",
    "    with open(path, 'r') as f:\n",
    "        word_gram = [line.strip('\\n') for line in f.readlines() if line]\n",
    "    return Counter(word_gram)\n",
    "word_gram = word_gram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_2_gram(path='word_2_gram.txt'):\n",
    "    with open(path, 'r') as f:\n",
    "        word_2_gram = [line.strip('\\n') for line in f.readlines() if line]\n",
    "    return Counter(word_2_gram)\n",
    "word_2_gram = word_2_gram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-gram p(w1|w2)P(w2|w3)\n",
    "def get_sentence_prod(sentence):\n",
    "    sent = my_cut(sentence)\n",
    "    prod = 1\n",
    "    for i, value in enumerate(sent[:-2]):\n",
    "        p = (word_2_gram[sent[i] + sent[i+1]]) / (word_gram[sent[i+1]] + 1)\n",
    "        prod *= p if p else 1/len(word_gram) # 如果 p概率为0 用 1/len(word_gram)代替 是否合适？ \n",
    "    return (sentence, prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(n=5, comments=None, grammer=None, host=None):\n",
    "    \"\"\"\n",
    "    n: 生成句子数\n",
    "    comments: 评论列表\n",
    "    grammer：自定义语法\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    if comments:\n",
    "        for comm in comments:\n",
    "            result = [get_sentence_prod(comm) for comm in comments if comm]\n",
    "    else:\n",
    "        for i in generate_n(grammer, host, n):\n",
    "            result.append(get_sentence_prod(i))\n",
    "    return sorted(result, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/nx/swsc48jj13gd4_bzw8vcr8lc0000gn/T/jieba.cache\n",
      "Loading model cost 1.117 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "best_comment = generate_best(grammer=grammer_rules, host='host')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('王宝强的恶心', 9.139013534879045e-05),\n",
       " ('王宝强的赌神稳住', 2.32347801734213e-05),\n",
       " ('吴京的杀破狼稳住', 9.323790990295336e-09),\n",
       " ('王宝强这部唐人街探案稳住', 8.325707705403082e-12),\n",
       " ('周润发这部杀破狼恶心', 1.5495454504751688e-12)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('还行吧一般般', 0.02242277633048009), ('吴京战狼两部曲搞的很不错', 1.8925943792974417e-26)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_best(comments=['吴京战狼两部曲搞的很不错', '还行吧一般般'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
