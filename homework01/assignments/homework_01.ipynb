{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二问 发送 mqgao@kaikeba.com\n",
    "2.1. what do you want to acquire in this course？\n",
    "\n",
    "自己之前学了几个月，看相关部分数学、数据结构、机器学习的基础算法，甚至有的部分看了几遍，但是没有相关的实践经验而且学的也不构系统，而且在没有实践应用的前提下会忘掉绝大部分。所以希望能够通过课程熟悉nlp的整个的实现流程，学好每个知识点，为转NLP打好基础。\n",
    "\n",
    "2.2. what problems do you want to solve？\n",
    "\n",
    "现在面临的就是业务逻辑和无止境的堆代码中，本身入行晚，只想通过这个课程早日转行成功\n",
    "\n",
    "2.3. what’s the advantages you have to finish you goal?\n",
    "\n",
    "相比而下92年出生可能年龄不算大，但是入行晚导致需要很多时间来补，所以觉得自己几乎没什么优势，不过优势不足勤奋来补嘛！\n",
    "\n",
    "2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "\n",
    "惰性真的是个可怕的事情，白天工作晚上学习真的很需要精力和毅力。\n",
    "克服惰性和提高专注力我觉得这两个是我目前继续提升的事情\n",
    "还有英语，真的是不能渣到在渣了\n",
    "\n",
    "2.5. How will you plan to study in this course period?\n",
    "\n",
    "晚上 跟老师上课，复习一下上课的内容，实践作业的内容\n",
    "早上会选择看英文paper 虽然英文不好每一句话都像天书，不过慢慢来，就是慢点而已"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础理论部分\n",
    "0. Can you come up out 3 sceneraies which use AI methods?\n",
    "Ans: {自动驾驶、智能家居、各类机器人}\n",
    "\n",
    "1. How do we use Github; Why do we use Jupyter and Pycharm;\n",
    "\n",
    "Ans: {github: 1. git add .;  2. git commit -m '123'; 3. git push origin your_branch 4.git pull oriign your_branch; 5 git clone git_address\n",
    "\n",
    "jupyter: 使用方便，简单直观可视化效果好。\n",
    "pyharm：强大的IDE, 貌似绝大部分的python开发者都用pycharm。}\n",
    "\n",
    "2. What's the Probability Model?\n",
    "\n",
    "Ans: 概率模型是指定随机变量出现的不确定性的概率。通过样本发生的概率判断其是否发生或产生相对正确的结果\n",
    "\n",
    "3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "\n",
    "Ans: 车牌摇号、文字纠错、谷歌搜索错误单词匹配、文本分类(垃圾邮件分类/文本分类等)(比如 jult --提示--> july)\n",
    "\n",
    "4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "\n",
    "Ans: 语言是千变万化、与时俱进的，使用正则匹配和语法无法满足现实需求或者是无法投入商业使用，而且后两者维护起来相对麻烦，对出现的新的语言或语句没有很好的处理方式\n",
    "\n",
    "5. What's the Language Model;\n",
    "\n",
    "Ans: 语言模型基于概率模型，在大量语料的基础上通过概率模型判断某一语句发生的可能性\n",
    "\n",
    "6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "\n",
    "Ans: 阿里/JD智能客服、语音识别、机器翻译\n",
    "\n",
    "7. What's the 1-gram language model;\n",
    "\n",
    "Ans: 1-gram 假设每个词的出现都是相互独立的，这样上下文对该词出现的概率没有影响\n",
    "\n",
    "8. What's the disadvantages and advantages of 1-gram language model;\n",
    "\n",
    "Ans: 1-gram模型简单计算概率时不需要考虑上下文因素，但是正因为如此可能导致上下文强相关词语的概率计算出现错误,亦或者多个特征对当前特征产生较大的影响会导致计算结果的严重偏差\n",
    "\n",
    "9. What't the 2-gram models;\n",
    "Ans: 在自然语言处理中，某一个特征出现的概率主要受其下一个特征值的影响，那么便知考虑下一个特征出现时当前特征出现的概率即p(w1|w2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我是一名健身爱好者，可以和你一起锻炼吗？\n",
    "我是一个健身教练，需要什么帮助吗\n",
    "我是一个职业健美人，我可以帮助你"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_grammer = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_grammer = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammer(gram, line_split='\\n', word_split='='):\n",
    "    grammer = {}\n",
    "    for line in gram.split(line_split):\n",
    "        if not line: continue\n",
    "        key, value = line.split(word_split)\n",
    "        grammer[''.join(key.split())] = [val.split() for val in value.split('|')]\n",
    "    return grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': [['自己', '寻找', '活动']],\n",
       " '自己': [['我'], ['俺'], ['我们']],\n",
       " '寻找': [['看看'], ['找找'], ['想找点']],\n",
       " '活动': [['乐子'], ['玩的']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_grammer(first_grammer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(sentence, target):\n",
    "    grammer = create_grammer(sentence)\n",
    "    if target not in grammer: return target\n",
    "    sen = [generate(sentence, t) for t in random.choice(grammer[target])]\n",
    "    return ''.join(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好我是9号,请问你要玩一玩打牌吗？'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(second_grammer, 'host')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(sentence, target, n):\n",
    "    for i in range(n):\n",
    "        yield generate(sentence, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女士,您好我是5493268号,请问你要玩一玩赌博吗？\n",
      "你好我是15号,请问你要玩一玩打牌吗？\n",
      "你好我是1278号,您需要耍一耍打猎吗？\n",
      "您好我是6867号,请问你要玩一玩喝酒吗？\n",
      "您好我是81号,请问你要耍一耍打牌吗？\n"
     ]
    }
   ],
   "source": [
    "for i in generate_n(second_grammer, 'host', 5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "content = pd.read_csv('/Users/haha/Desktop/Documents/datasource-master/movie_comments.csv')\n",
    "comment = content['comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_txt(string):\n",
    "    return re.findall('\\w+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment = [''.join(deal_txt(string)) for string in comment]\n",
    "new_comment = []\n",
    "for s in comment:\n",
    "    if not s and not instance(s, 'str'): continue\n",
    "        \n",
    "    with open('new_comment.txt', 'a') as f:\n",
    "        try:\n",
    "            f.write(''.join(deal_txt(s))+'\\n')\n",
    "        except Exception as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261495\n"
     ]
    }
   ],
   "source": [
    "with open('new_comment.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cut(string):\n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_comment.txt', 'r') as fr:\n",
    "    with open('word_gram.txt', 'a') as fw:\n",
    "        for line in fr.readlines():\n",
    "            for l in my_cut(line):\n",
    "                fw.write(l +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5013299\n",
      "5013298\n",
      "恶心\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-ec91f01a9ab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0mfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-ec91f01a9ab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "with open('word_gram.txt', 'r') as fr:\n",
    "    with open('word_2_gram.txt', 'a') as fw:\n",
    "        readlines = fr.readlines()[:-2]\n",
    "        print(len(readlines))\n",
    "        for i in range(len(readlines)):\n",
    "            try:\n",
    "                fw.write(readlines[i].strip('\\n') + readlines[i+1].strip('\\n')+'\\n')\n",
    "            except Exception as e:\n",
    "                print(i)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_gram.txt', 'r') as f:\n",
    "    word_gram = [line.strip('\\n') for line in f.readlines() if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_2_gram.txt', 'r') as f:\n",
    "    word_2_gram = [line.strip('\\n') for line in f.readlines() if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_gram = Counter(word_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_gram = Counter(word_2_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328262"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_gram['的']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-gram p(w1|w2)P(w2|w3)\n",
    "def get_sentence_prod(sentence):\n",
    "    sent = my_cut(sentence)\n",
    "    prod = 1\n",
    "    for i, value in enumerate(sent[:-2]):\n",
    "        p = (word_2_gram[sent[i] + sent[i+1]]) / (word_gram[sent[i+1]] + 1)\n",
    "        prod *= p if p else 1/len(word_gram) # 如果 p概率为0 用 1/len(word_gram)代替 是否合适？ \n",
    "    return (sentence, prod)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('这个电影太好看了', 2.8532810936066384e-06)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_prod('这个电影太好看了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_grammer = \"\"\"\n",
    "host = 主演 电影 评论\n",
    "主演 = 吴京 adj | 王宝强 adj | 周润发 adj\n",
    "adj = 的 | 这部 | 这个 | 这辆\n",
    "电影 = 杀破狼 | 战狼 | 唐人街探案 | 赌神 |\n",
    "评论 = 太好了 | 垃圾 | 恶心 | 好吃 | 车 adj | 稳住 | 不错\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(n=5, comments=None, grammer=None, host=None):\n",
    "    \"\"\"\n",
    "    n: 生成句子数\n",
    "    comments: 评论列表\n",
    "    grammer：自定义语法\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    if comments:\n",
    "        for comm in comments:\n",
    "            result = [get_sentence_prod(comm) for comm in comments if comm]\n",
    "    else:\n",
    "        for i in generate_n(grammer, host, 5):\n",
    "            result.append(get_sentence_prod(i))\n",
    "    return sorted(result, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('俺看看乐子', 6.083021071584992e-06), ('我们看看乐子', 6.083021071584992e-06), ('我们看看乐子', 6.083021071584992e-06), ('我们想找点乐子', 8.9900889140716e-09), ('俺想找点玩的', 1.42482707526176e-11)]\n"
     ]
    }
   ],
   "source": [
    "print(generate_best(grammer=first_grammer, host='human'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = ['战狼2,吴京意淫到了脑残的地步，看了恶心想吐', \n",
    "        '吴京比这句话还要意淫一百倍', \n",
    "        '脑子是个好东西，希望编剧们都能有',\n",
    "        '战狼2,心往一处想，劲往一处使，就能实现我们的梦想。看吧，比第一部好太多了',\n",
    "        '打戏非常带感，燃爆了，拳拳到肉，看得超爽']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('吴京比这句话还要意淫一百倍', 2.5126150626694205e-13), ('脑子是个好东西，希望编剧们都能有', 2.9374604576472745e-27), ('打戏非常带感，燃爆了，拳拳到肉，看得超爽', 9.091955759826785e-44), ('战狼2,吴京意淫到了脑残的地步，看了恶心想吐', 7.402218092467513e-44), ('战狼2,心往一处想，劲往一处使，就能实现我们的梦想。看吧，比第一部好太多了', 4.024219045176691e-80)]\n"
     ]
    }
   ],
   "source": [
    "print(generate_best(comments=comm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
