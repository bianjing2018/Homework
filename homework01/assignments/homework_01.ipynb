{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二问 发送 mqgao@kaikeba.com\n",
    "2.1. what do you want to acquire in this course？\n",
    "\n",
    "自己之前学了几个月，看相关部分数学、数据结构、机器学习的基础算法，甚至有的部分看了几遍，但是没有相关的实践经验而且学的也不构系统，而且在没有实践应用的前提下会忘掉绝大部分。所以希望能够通过课程熟悉nlp的整个的实现流程，学好每个知识点，为转NLP打好基础。\n",
    "2.2. what problems do you want to solve？\n",
    "    \n",
    "2.3. what’s the advantages you have to finish you goal?\n",
    "\n",
    "2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "\n",
    "2.5. How will you plan to study in this course period?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础理论部分\n",
    "0. Can you come up out 3 sceneraies which use AI methods?\n",
    "Ans: {自动驾驶、智能家居、各类机器人}\n",
    "\n",
    "1. How do we use Github; Why do we use Jupyter and Pycharm;\n",
    "\n",
    "Ans: {github: 1. git add .;  2. git commit -m '123'; 3. git push origin your_branch 4.git pull oriign your_branch; 5 git clone git_address\n",
    "\n",
    "jupyter: 使用方便，简单直观可视化效果好。\n",
    "pyharm：强大的IDE, 貌似绝大部分的python开发者都用pycharm。}\n",
    "\n",
    "2. What's the Probability Model?\n",
    "\n",
    "Ans: 概率模型是指定随机变量出现的不确定性的概率。通过样本发生的概率判断其是否发生或产生相对正确的结果\n",
    "\n",
    "3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "\n",
    "Ans: 车牌摇号、文字纠错、谷歌搜索错误单词匹配、文本分类(垃圾邮件分类/文本分类等)(比如 jult --提示--> july)\n",
    "\n",
    "4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "\n",
    "Ans: 语言是千变万化、与时俱进的，使用正则匹配和语法无法满足现实需求或者是无法投入商业使用，而且后两者维护起来相对麻烦，对出现的新的语言或语句没有很好的处理方式\n",
    "\n",
    "5. What's the Language Model;\n",
    "\n",
    "Ans: 语言模型基于概率模型，在大量语料的基础上通过概率模型判断某一语句发生的可能性\n",
    "\n",
    "6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "\n",
    "Ans: 阿里/JD智能客服、语音识别、机器翻译\n",
    "\n",
    "7. What's the 1-gram language model;\n",
    "\n",
    "Ans: \n",
    "\n",
    "8. What's the disadvantages and advantages of 1-gram language model;\n",
    "\n",
    "Ans:\n",
    "\n",
    "9. What't the 2-gram models;\n",
    "Ans:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我是一名健身爱好者，可以和你一起锻炼吗？\n",
    "我是一个健身教练，需要什么帮助吗\n",
    "我是一个职业健美人，我可以帮助你"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_grammer = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_grammer = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammer(gram, line_split='\\n', word_split='='):\n",
    "    grammer = {}\n",
    "    for line in gram.split(line_split):\n",
    "        if not line: continue\n",
    "        key, value = line.split(word_split)\n",
    "        grammer[''.join(key.split())] = [val.split() for val in value.split('|')]\n",
    "    return grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': [['自己', '寻找', '活动']],\n",
       " '自己': [['我'], ['俺'], ['我们']],\n",
       " '寻找': [['看看'], ['找找'], ['想找点']],\n",
       " '活动': [['乐子'], ['玩的']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_grammer(first_grammer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(sentence, target):\n",
    "    grammer = create_grammer(sentence)\n",
    "    if target not in grammer: return target\n",
    "    sen = [generate(sentence, t) for t in random.choice(grammer[target])]\n",
    "    return ''.join(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小朋友,您好我是25号,您需要玩一玩赌博吗？'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(second_grammer, 'host')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(sentence, target, n):\n",
    "    for i in range(n):\n",
    "        yield generate(sentence, target)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "先生,您好我是22号,您需要玩一玩喝酒吗？\n",
      "你好我是1868号,您需要耍一耍喝酒吗？\n",
      "女士,你好我是534号,您需要玩一玩打牌吗？\n",
      "先生,您好我是3号,请问你要耍一耍打猎吗？\n",
      "先生,您好我是37号,请问你要玩一玩赌博吗？\n"
     ]
    }
   ],
   "source": [
    "for i in generate_n(second_grammer, 'host', 5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吴京意淫到了脑残的地步，看了恶心想吐', '首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮番上场，视物理逻辑于不顾，不得不说有钱真好，随意胡闹', '吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋律，为了煽情而煽情，让人觉得他是个大做作、大谎言家。（7.29更新）片子整体不如湄公河行动，1.整体不够流畅，编剧有毒，台词尴尬；2.刻意做作的主旋律煽情显得如此不合时宜而又多余。', '凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。', '中二得很', '“犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。', '脑子是个好东西，希望编剧们都能有。', '三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心，他终于抛弃李忠志了，新增外来班底让硬件实力有机会和国际接轨，开篇水下长镜头和诸如铁丝网拦截RPG弹头的细节设计都让国产动作片重新封顶，在理念上，它甚至做到《绣春刀2》最想做到的那部分。', '开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹簧床架挡炸弹 空手接碎玻璃 弹匣割喉等帅得飞起！就算前半段铺垫节奏散漫主角光环开太大等也不怕 作为一个中国人 两个小时弥漫着中国强大得不可侵犯的氛围 还是让那颗民族自豪心砰砰砰跳个不停。', '15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个人，无能的政府需要求助于这些英雄才能解决难题，体现的是个人的价值，所以主旋律照抄这种模式实际上是有问题的。我们以前嘲笑个人英雄主义，却没想到捆绑爱国主义的全能战士更加难以下咽。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "content = pd.read_csv('/Users/bj/Desktop/Documents/datasource-master/movie_comments.csv')\n",
    "comment = content['comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_txt(string):\n",
    "    return re.findall('\\w+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment = [''.join(deal_txt(string)) for string in comment]\n",
    "new_comment = []\n",
    "for s in comment:\n",
    "    if not s and not instance(s, 'str'): continue\n",
    "        \n",
    "    with open('new_comment.txt', 'a') as f:\n",
    "        try:\n",
    "            f.write(''.join(deal_txt(s))+'\\n')\n",
    "        except Exception as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261495\n"
     ]
    }
   ],
   "source": [
    "with open('new_comment.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cut(string):\n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/lr/6fq1pq79667bzkg9qx8smrk40000gn/T/jieba.cache\n",
      "Loading model cost 0.828 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['中华人民共和国', '成立', '了']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cut('中华人民共和国成立了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_comment.txt', 'r') as fr:\n",
    "    with open('word_gram.txt', 'a') as fw:\n",
    "        for line in fr.readlines():\n",
    "            for l in my_cut(line):\n",
    "                fw.write(l +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5013299\n",
      "5013298\n",
      "恶心\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-ec91f01a9ab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0mfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-ec91f01a9ab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "with open('word_gram.txt', 'r') as fr:\n",
    "    with open('word_2_gram.txt', 'a') as fw:\n",
    "        readlines = fr.readlines()[:-2]\n",
    "        print(len(readlines))\n",
    "        for i in range(len(readlines)):\n",
    "            try:\n",
    "                fw.write(readlines[i].strip('\\n') + readlines[i+1].strip('\\n')+'\\n')\n",
    "            except Exception as e:\n",
    "                print(i)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_gram.txt', 'r') as f:\n",
    "    word_gram = [line.strip('\\n') for line in f.readlines() if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_2_gram.txt', 'r') as f:\n",
    "    word_2_gram = [line.strip('\\n') for line in f.readlines() if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_gram = Counter(word_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_gram = Counter(word_2_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328262"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_gram['的']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-gram p(w1|w2)P(w2|w3)\n",
    "def get_sentence_prod(sentence):\n",
    "    sent = my_cut(sentence)\n",
    "    prod = 1\n",
    "    print(sent)\n",
    "    for i, value in enumerate(sent[:-2]):\n",
    "        p = (word_2_gram[sent[i] + sent[i+1]]) / (word_gram[sent[i+1]] + 1)\n",
    "        print('{}--->{}'.format(sent[i] + sent[i+1], str(word_2_gram[sent[i] + sent[i+1]])))\n",
    "        print('{}--->{}'.format(sent[i+1], str(word_gram[sent[i+1]])))\n",
    "        print(p)\n",
    "        prod *= p if p else 1/len(word_gram)\n",
    "    return prod\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吴京', '的', '电影', '一般般']\n",
      "吴京的--->50\n",
      "的--->328262\n",
      "0.0001523168922479841\n",
      "的电影--->8604\n",
      "电影--->33675\n",
      "0.25549352654709584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.891597995313146e-05"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_prod('吴京的电影一般般')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['武警', '的', '电影', '一般般']\n",
      "武警的--->0\n",
      "的--->328262\n",
      "0.0\n",
      "的电影--->8604\n",
      "电影--->33675\n",
      "0.25549352654709584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5541725056395435e-06"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_prod('武警的电影一般般')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['明天', '好不好', '玩', '呢']\n",
      "明天好不好--->0\n",
      "好不好--->191\n",
      "0.0\n",
      "好不好玩--->0\n",
      "玩--->1220\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.700314535734703e-11"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_prod('明天好不好玩呢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今天', '真好', '花']\n",
      "今天真好--->0\n",
      "真好--->190\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.083021071584992e-06"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_prod('今天真好花')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这个', '电影', '太', '好看', '了']\n",
      "这个电影--->525\n",
      "电影--->33675\n",
      "0.015589737498515263\n",
      "电影太--->73\n",
      "太--->12676\n",
      "0.005758460203518182\n",
      "太好看--->257\n",
      "好看--->8085\n",
      "0.031783329210981946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.8532810936066384e-06"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_prod('这个电影太好看了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
